{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb86ca3-85fe-4e1b-8057-27dd63227ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex: [0. 1.]\n",
      "cp: [1. 2. 3. 4.]\n",
      "fbs: [0. 1.]\n",
      "restecg: [0. 1. 2.]\n",
      "exang: [0. 1.]\n",
      "slope: [1. 2. 3.]\n",
      "thal: [3. 6. 7.]\n",
      "Sample count for each label:\n",
      "0.0, 128\n",
      "1.0, 41\n",
      "2.0, 30\n",
      "3.0, 30\n",
      "4.0, 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = np.load('features.npz')\n",
    "xtrain = data['xtrain']\n",
    "ytrain = data['ytrain']\n",
    "\n",
    "# Check the distinct values in each categorical column\n",
    "categorical_cols = [1, 2, 5, 6, 8, 10, 12]  # Indices for categorical features\n",
    "col_names = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "for i, col_idx in enumerate(categorical_cols):\n",
    "    unique_vals = np.unique(xtrain[:, col_idx])\n",
    "    print(f\"{col_names[i]}: {unique_vals}\")\n",
    "\n",
    "# Print counts for each label\n",
    "values, counts = np.unique(ytrain, return_counts = True)\n",
    "print(\"Sample count for each label:\")\n",
    "for i, c in zip(values, counts):\n",
    "    print(f\"{i}, {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ebe16e6-9c21-40e3-b1b0-f615499585dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function concept for data preprocessing\n",
    "# Work in progress!\n",
    "def preprocess_data(X_train):\n",
    "    # Feature values as seen in the block above\n",
    "    categorical_features = {\n",
    "        'categorical_features_indices': [1, 2, 5, 6, 8, 10, 12],\n",
    "        'categorical_features_names': ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal'],\n",
    "        'categorical_features_values': [\n",
    "            [0, 1],              # sex values\n",
    "            [1, 2, 3, 4],        # cp values\n",
    "            [0, 1],              # fbs values \n",
    "            [0, 1, 2],           # restecg values\n",
    "            [0, 1],              # exang values\n",
    "            [1, 2, 3],           # slope values\n",
    "            [3, 6, 7]            # thal values\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    numerical_features = {\n",
    "        'numerical_features_indices': [0, 3, 4, 7, 9, 11],\n",
    "        'numerical_features_names': ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "    }\n",
    "    \n",
    "    # Processing\n",
    "    # Integer types: as seen in exercise series: subtract expectation, divide by standard deviation\n",
    "    X_numerical_train = X_train[:, numerical_features['numerical_features_indices']]\n",
    "    X_train_mean = np.mean(X_numerical_train, axis=0)\n",
    "    X_train_std = np.std(X_numerical_train, axis=0)\n",
    "    X_numerical_train_scaled = (X_numerical_train - X_train_mean)/X_train_std\n",
    "    \n",
    "    # Now we have arrays containing preprocessed integer types - they should be \"merged\" with categorical features somehow.\n",
    "\n",
    "    # Categorical types: a reasonable course of action is to do one-hot encoding\n",
    "    # This would ensure there are less/no problems with predictions skewing towards greater values (in abs. value)\n",
    "\n",
    "    X_categorical_train = X_train[:, categorical_features['categorical_features_indices']]\n",
    "\n",
    "    # Initiate returned values to the numerical processed values\n",
    "    processed_X_train = X_numerical_train_scaled\n",
    "    \n",
    "    # One-hot encoding process for categorical values\n",
    "   \n",
    "    # Create tuple list [(0, 1), (1, 2), (2, 5), ...] where the first element is the index i, and the second is the dataset index idx taken from\n",
    "    # the element categorical_features_indices\n",
    "    for i, idx in enumerate(categorical_features['categorical_features_indices']):\n",
    "        # Get the corresponding [a, b, c, ...] in the categorical feature values\n",
    "        possible_values = categorical_features['categorical_features_values'][i]\n",
    "        \n",
    "        # One-hot encoding creation\n",
    "        for value in possible_values:\n",
    "            # Create binary features (1 if the feature equals this value, 0 otherwise), by iterating over every possible value and applying mask\n",
    "            # Done column per column! TODO: For binary categories, only create one column!\n",
    "            bin_values_train = (X_train[:, idx] == value).astype(float).reshape(-1, 1)\n",
    "            \n",
    "            # Horizontally stacking the processed column to create one-hot encoding for categorical feature at idx\n",
    "            processed_X_train = np.hstack((processed_X_train, bin_values_train))\n",
    "    \n",
    "    return processed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeef099f-691b-4e11-bfe6-b27b9bad6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given dataset contains unbalanced counts belonging to each class.\n",
    "# We should equalise their impact in some way - as seen in k-NN, we normalise\n",
    "# using a weight value computed from the frequency of each class.\n",
    "# Quote k-NN slide 35: \"Weighing neighbors by the inverse of their class size converts neighbor counts\n",
    "# into the fraction of each class that falls in your K nearest neighbors.\"\n",
    "def calculate_sample_weights(y_train):\n",
    "    # Class frequency computation\n",
    "    classes = np.unique(y_train) # Should be [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "    n_samples = len(y_train)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # [0, 1, 2, 3, 4] yields counts [128, 41, 30, 30, 8] (see above, first cell)\n",
    "    class_counts = np.bincount(y_train.astype(int))\n",
    "    \n",
    "    # Calculate weights inversely proportional to class frequency\n",
    "    # Rare classes get higher weights\n",
    "    class_weights = n_samples / class_counts  # elem-wise division, yields vector (if I'm not mistaken)\n",
    "\n",
    "    \n",
    "    # Map each sample to its corresponding class weight\n",
    "    sample_weights = np.array([class_weights[int(label)] for label in y_train])\n",
    "    \n",
    "    return sample_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
