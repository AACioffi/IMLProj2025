{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5296cd-6d16-4443-9cdb-e8b2829ec1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting medmnist\n",
      "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (2.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (1.6.1)\n",
      "Collecting scikit-image (from medmnist)\n",
      "  Downloading scikit_image-0.25.2-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (4.67.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (11.1.0)\n",
      "Collecting fire (from medmnist)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from medmnist) (0.20.1)\n",
      "Collecting termcolor (from fire->medmnist)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from pandas->medmnist) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from pandas->medmnist) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from pandas->medmnist) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from scikit-image->medmnist) (1.15.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from scikit-image->medmnist) (3.4.2)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->medmnist)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->medmnist)\n",
      "  Downloading tifffile-2025.5.21-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from scikit-image->medmnist) (24.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->medmnist)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from scikit-learn->medmnist) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from scikit-learn->medmnist) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from torch->medmnist) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from torch->medmnist) (4.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from torch->medmnist) (72.1.0)\n",
      "Collecting sympy==1.13.1 (from torch->medmnist)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from torch->medmnist) (3.1.5)\n",
      "Collecting fsspec (from torch->medmnist)\n",
      "  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from tqdm->medmnist) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexf\\miniconda3\\lib\\site-packages (from jinja2->torch->medmnist) (3.0.2)\n",
      "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
      "Downloading scikit_image-0.25.2-cp312-cp312-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.6/12.9 MB 8.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.1/12.9 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.9 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.9 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.2 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 10.0 MB/s eta 0:00:00\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.5.21-py3-none-any.whl (229 kB)\n",
      "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114263 sha256=deffdd6d3c368741ff8de8515f8d1463d4e69be6a8f25e017e9e7b312772d3f4\n",
      "  Stored in directory: c:\\users\\alexf\\appdata\\local\\pip\\cache\\wheels\\9e\\5b\\45\\29f72e55d87a29426b04b3cfdf20325c079eb97ab74f59017d\n",
      "Successfully built fire\n",
      "Installing collected packages: tifffile, termcolor, sympy, lazy-loader, imageio, fsspec, scikit-image, fire, medmnist\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed fire-0.7.0 fsspec-2025.5.0 imageio-2.37.0 lazy-loader-0.4 medmnist-3.0.2 scikit-image-0.25.2 sympy-1.13.1 termcolor-3.1.0 tifffile-2025.5.21\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell to install medmnist if not present\n",
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c414daa6-757b-4afa-829f-a73e845fc63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\alexf\\.medmnist\\dermamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\alexf\\.medmnist\\dermamnist.npz\n",
      "Train images shape -- N_tr = 7007, H_tr = 28, W_tr = 28, C_tr = 3\n",
      "Test images shape  -- N_te = 2005, H_te = 28, W_te = 28, C_te = 3\n",
      "Classes:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Number of classes: 7\n",
      "Class counts: [ 228  359  769   80  779 4693   99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data import load_data\n",
    "\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = load_data()\n",
    "\n",
    "# Checking dimemsions\n",
    "N_tr, H_tr, W_tr, C_tr = train_images.shape\n",
    "N_te, H_te, W_te, C_te = test_images.shape\n",
    "\n",
    "print(f'Train images shape -- N_tr = {N_tr}, H_tr = {H_tr}, W_tr = {W_tr}, C_tr = {C_tr}')\n",
    "print(f'Test images shape  -- N_te = {N_te}, H_te = {H_te}, W_te = {W_te}, C_te = {C_te}')\n",
    "\n",
    "# Loading data to examine classes\n",
    "extracted_classes = np.unique(train_labels)\n",
    "n_classes = len(extracted_classes)\n",
    "\n",
    "class_counts = np.bincount(train_labels.astype(int))\n",
    "\n",
    "print(f'Classes:')\n",
    "for c in extracted_classes:\n",
    "    print(f'{c}')\n",
    "print(f'Number of classes: {n_classes}')\n",
    "print(f'Class counts: {class_counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a662e488-08f3-4930-8165-e5358dfe4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_mlp(train_images, test_images, train_labels, test_labels):\n",
    "    \"\"\"\n",
    "    Preprocesses image data for MLP training\n",
    "    \n",
    "    Performs the following operations:\n",
    "    1. Flattens 28x28x3 images into 1D vectors (2352 features)\n",
    "    2. Normalizes pixel values from [0,255] to [0,1] range\n",
    "    3. Creates stratified train/validation split (80/20) maintaining class proportions\n",
    "    4. Calculates normalized class weights for handling imbalanced dataset\n",
    "    \n",
    "    Args:\n",
    "        train_images: Training images array of shape (N, 28, 28, 3)\n",
    "        test_images:  Test images array of shape (N_test, 28, 28, 3)\n",
    "        train_labels: Training labels array of shape (N,)\n",
    "        test_labels:  Test labels array of shape (N_test,)\n",
    "    \n",
    "    Returns:\n",
    "        train_set_datapoints:      Flattened, normalized training data (80% of original)\n",
    "        train_set_labels:          Corresponding training labels\n",
    "        validation_set_datapoints: Flattened, normalized validation data (20% of original)\n",
    "        validation_set_labels:     Corresponding validation labels\n",
    "        proc_test_images:          Flattened, normalized test data\n",
    "        test_labels:               Original test labels (unchanged)\n",
    "        normalised_weights:        Class weights for loss function (shape: n_classes,)\n",
    "    \"\"\"\n",
    "    # Image dimensions\n",
    "    N_tr, H_tr, W_tr, C_tr = train_images.shape\n",
    "    N_te, H_te, W_te, C_te = test_images.shape\n",
    "    \n",
    "    # Image flattening\n",
    "    proc_train_images = train_images.reshape(N_tr, -1).astype(np.float32)\n",
    "    proc_test_images  = test_images.reshape(N_te, -1).astype(np.float32)\n",
    "    \n",
    "    # Normalisation (min-max scaling - prevents negative values, maps 0-255 to 0-1)\n",
    "    proc_train_images = proc_train_images / 255.0\n",
    "    proc_test_images  = proc_test_images  / 255.0\n",
    "    \n",
    "    # Get number of classes\n",
    "    n_classes = len(np.unique(train_labels))\n",
    "    \n",
    "    # Training, Validation set construction\n",
    "    # Take 20% from each class to preserve proportions (for validation)\n",
    "    p_validation = 0.2\n",
    "    \n",
    "    # Use lists for collecting data\n",
    "    train_images_list = []\n",
    "    train_labels_list = []\n",
    "    validation_images_list = []\n",
    "    validation_labels_list = []\n",
    "    \n",
    "    for class_index in range(n_classes):\n",
    "        # Retrieving datapoint indices in train_images with given class index\n",
    "        class_indices = np.where(train_labels == class_index)[0]\n",
    "        \n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Calculate split point\n",
    "        n_validation = int(len(class_indices) * p_validation)\n",
    "        \n",
    "        # Split indices\n",
    "        validation_indices = class_indices[:n_validation]\n",
    "        training_indices   = class_indices[n_validation:]\n",
    "        \n",
    "        # Append to lists\n",
    "        validation_images_list.append(proc_train_images[validation_indices])\n",
    "        validation_labels_list.append(train_labels[validation_indices])\n",
    "        train_images_list.append(proc_train_images[training_indices])\n",
    "        train_labels_list.append(train_labels[training_indices])\n",
    "    \n",
    "    # Concatenate all lists\n",
    "    train_set_datapoints      = np.vstack(train_images_list)\n",
    "    train_set_labels          = np.concatenate(train_labels_list)\n",
    "    validation_set_datapoints = np.vstack(validation_images_list)\n",
    "    validation_set_labels     = np.concatenate(validation_labels_list)\n",
    "    \n",
    "    # Class weight calculation (using the training set after split)\n",
    "    class_counts       = np.bincount(train_set_labels.astype(int))\n",
    "    total_samples      = len(train_set_labels)\n",
    "    raw_weights        = total_samples / class_counts\n",
    "    normalised_weights = raw_weights * (n_classes / np.sum(raw_weights))\n",
    "    \n",
    "    return train_set_datapoints, train_set_labels, validation_set_datapoints, validation_set_labels, proc_test_images, test_labels, normalised_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
